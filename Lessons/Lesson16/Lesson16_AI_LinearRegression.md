# <center>**[Cool Kids Coding School](https://www.coolkidscodingschool.com)**</center>

---
## Problem Solving with Python<br> Lesson 16: Introduction to AI (Linear Regression)

![logo](./images/ckcslogo.png)

---
## Regression
Regression analysis is one of the most important fields in statistics and machine learning. There are many regression methods available. In previous lessons we discussed Logistic regression used in classification problems.  Today we are going to discuss Linear regression.

![xkcd](./images/machine_learning.png)

> 1.0 What is a regression?

Regression searches for relationships among variables.

For example, you can observe several employees of some company and try to understand how their salaries depend on the features, such as experience, level of education, role, city they work in, and so on.

This is a regression problem where data related to each employee represent one observation. The presumption is that the experience, education, role, and city are the independent features, while the salary depends on them.

Similarly, you can try to establish a mathematical dependence of the prices of houses on their areas, numbers of bedrooms, distances to the city center, and so on.

Generally, in regression analysis, you usually consider some phenomenon of interest and have a number of observations. Each observation has two or more features. Following the assumption that (at least) one of the features depends on the others, you try to establish a relation among them.

In other words, you need to find a function that maps some features or variables to others sufficiently well.

The dependent features are called the dependent variables, outputs, or responses.

The independent features are called the independent variables, inputs, or predictors.

Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.

It is a common practice to denote the outputs with ð‘¦ and inputs with ð‘¥. If there are two or more independent variables, they can be represented as the vector ð± = (ð‘¥â‚, â€¦, ð‘¥áµ£), where ð‘Ÿ is the number of inputs.

> 2.0 What is a Linear Regressions

When implementing linear regression of some dependent variable ð‘¦ on the set of independent variables ð± = (ð‘¥â‚, â€¦, ð‘¥áµ£), where ð‘Ÿ is the number of predictors, you assume a linear relationship between ð‘¦ and ð±: ð‘¦ = ð›½â‚€ + ð›½â‚ð‘¥â‚ + â‹¯ + ð›½áµ£ð‘¥áµ£ + ðœ€. This equation is the regression equation. ð›½â‚€, ð›½â‚, â€¦, ð›½áµ£ are the regression coefficients, and ðœ€ is the random error.

Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with ð‘â‚€, ð‘â‚, â€¦, ð‘áµ£. They define the estimated regression function ð‘“(ð±) = ð‘â‚€ + ð‘â‚ð‘¥â‚ + â‹¯ + ð‘áµ£ð‘¥áµ£. This function should capture the dependencies between the inputs and output sufficiently well.

The estimated or predicted response, ð‘“(ð±áµ¢), for each observation ð‘– = 1, â€¦, ð‘›, should be as close as possible to the corresponding actual response ð‘¦áµ¢. The differences ð‘¦áµ¢ - ð‘“(ð±áµ¢) for all observations ð‘– = 1, â€¦, ð‘›, are called the residuals. Regression is about determining the best predicted weights, that is the weights corresponding to the smallest residuals.

To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations ð‘– = 1, â€¦, ð‘›: SSR = Î£áµ¢(ð‘¦áµ¢ - ð‘“(ð±áµ¢))Â². This approach is called the method of ordinary least squares.

Root mean squared error (RMSE): RMSE is a quadratic scoring rule that also measures the average magnitude of the error. Itâ€™s the square root of the average of squared differences between prediction and actual observation.

![RMSE](./images/rmse.gif)

![Single Variable Regression](./images/regression_line.png)

> 3.0 Regression Performance

The coefficient of determination, denoted as ð‘…Â², tells you which amount of variation in ð‘¦ can be explained by the dependence on ð± using the particular regression model. Larger ð‘…Â² indicates a better fit and means that the model can better explain the variation of the output with different inputs.

The value ð‘…Â² = 1 corresponds to SSR = 0, that is to the perfect fit since the values of predicted and actual responses fit completely to each other.

![Regression Fitting](./images/fitting.gif)

> 4.0 Underfitting and Overfitting

One very important question that might arise when youâ€™re implementing polynomial regression is related to the choice of the optimal degree of the polynomial regression function.

There is no straightforward rule for doing this. It depends on the case. You should, however, be aware of two problems that might follow the choice of the degree: underfitting and overfitting.

Underfitting occurs when a model canâ€™t accurately capture the dependencies among data, usually as a consequence of its own simplicity. It often yields a low ð‘…Â² with known data and bad generalization capabilities when applied with new data.

Overfitting happens when a model learns both dependencies among data and random fluctuations. In other words, a model learns the existing data too well. Complex models, which have many features or terms, are often prone to overfitting. When applied to known data, such models usually yield high ð‘…Â². However, they often donâ€™t generalize well and have significantly lower ð‘…Â² when used with new data.

The next figure illustrates the underfitted, well-fitted, and overfitted models:

![Over and Under](./images/over_under.webp)

> 5.0 Example

Letâ€™s start with the simplest case, which is simple linear regression.

There are five basic steps when youâ€™re implementing linear regression:

    Import the packages and classes you need.
    Provide data to work with and eventually do appropriate transformations.
    Create a regression model and fit it with existing data.
    Check the results of model fitting to know whether the model is satisfactory.
    Apply the model for predictions.

These steps are more or less general for most of the regression approaches and implementations.

>> Step 1: Import packages and classes

The first step is to import the package numpy and the class LinearRegression from sklearn.linear_model:

```python
import numpy as np
from sklearn.linear_model import LinearRegression
```

Now, you have all the functionalities you need to implement linear regression.

>> Step 2: Provide data

The second step is defining data to work with. The inputs (regressors, ð‘¥) and output (predictor, ð‘¦) should be arrays (the instances of the class numpy.ndarray) or similar objects. This is the simplest way of providing data for regression:

```python
x = np.array([5, 15, 25, 35, 45, 55]).reshape((-1, 1))
y = np.array([5, 20, 14, 32, 22, 38])
```

Now, you have two arrays: the input x and output y. You should call .reshape() on x because this array is required to be two-dimensional, or to be more precise, to have one column and as many rows as necessary. Thatâ€™s exactly what the argument (-1, 1) of .reshape() specifies.

This is how x and y look now:

```python
>>> print(x)
[[ 5]
 [15]
 [25]
 [35]
 [45]
 [55]]
>>> print(y)
[ 5 20 14 32 22 38]
```

As you can see, x has two dimensions, and x.shape is (6, 1), while y has a single dimension, and y.shape is (6,).

>> Step 3: Create a model and fit it

The next step is to create a linear regression model and fit it using the existing data.

Letâ€™s create an instance of the class LinearRegression, which will represent the regression model:

```python
model = LinearRegression()
```

This statement creates the variable model as the instance of LinearRegression. You can provide several optional parameters to LinearRegression:

Itâ€™s time to start using the model. First, you need to call .fit() on model:
```python
model.fit(x, y)
```
With .fit(), you calculate the optimal values of the weights ð‘â‚€ and ð‘â‚, using the existing input and output (x and y) as the arguments. In other words, .fit() fits the model. It returns self, which is the variable model itself. Thatâ€™s why you can replace the last two statements with this one:

```python
model = LinearRegression().fit(x, y)
```

This statement does the same thing as the previous two. Itâ€™s just shorter.

>> Step 4: Get results

Once you have your model fitted, you can get the results to check whether the model works satisfactorily and interpret it.

You can obtain the coefficient of determination (ð‘…Â²) with .score() called on model:

```python
>>> r_sq = model.score(x, y)
>>> print('coefficient of determination:', r_sq)
coefficient of determination: 0.715875613747954
```

When youâ€™re applying .score(), the arguments are also the predictor x and regressor y, and the return value is ð‘…Â².

The attributes of model are .intercept_, which represents the coefficient, ð‘â‚€ and .coef_, which represents ð‘â‚:

```python
>>> print('intercept:', model.intercept_)
intercept: 5.633333333333329
>>> print('slope:', model.coef_)
slope: [0.54]
```

The code above illustrates how to get ð‘â‚€ and ð‘â‚. You can notice that .intercept_ is a scalar, while .coef_ is an array.

The value ð‘â‚€ = 5.63 (approximately) illustrates that your model predicts the response 5.63 when ð‘¥ is zero. The value ð‘â‚ = 0.54 means that the predicted response rises by 0.54 when ð‘¥ is increased by one.

>> Step 5: Predict response

Once there is a satisfactory model, you can use it for predictions with either existing or new data.

To obtain the predicted response, use .predict():

```python
>>> y_pred = model.predict(x)
>>> print('predicted response:', y_pred, sep='\n')
predicted response:
[ 8.33333333 13.73333333 19.13333333 24.53333333 29.93333333 35.33333333]
```

When applying .predict(), you pass the regressor as the argument and get the corresponding predicted response.



> 6.0 Today's Problem

In today's problem we are going to try to predict housing prices in Boston.  We are going to use a dataset that describes various house features and the price of the house.

---

## **Any Questions?**

### **for any questions contact hw_help@coolkidscodingschool.com**
